{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wohecha/HuggingFace-Unit1/blob/main/notebooks/unit5/unit5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D3NL_e4crQv"
      },
      "source": [
        "# Unit 5: An Introduction to ML-Agents\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For Pyramids: Mean Reward = 1.75  \n",
        "- For SnowballTarget: Mean Reward = 15 or 30 targets shoot in an episode."
      ],
      "metadata": {
        "id": "1o5UD5BAuq06"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-cYE0K5iL-w"
      },
      "source": [
        "### üéÆ Environments:\n",
        "\n",
        "- [Pyramids](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#pyramids)\n",
        "- SnowballTarget\n",
        "\n",
        "### üìö RL-Library:\n",
        "\n",
        "- [ML-Agents](https://github.com/Unity-Technologies/ml-agents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEhtaFh9i31S"
      },
      "source": [
        "We're constantly trying to improve our tutorials, so **if you find some issues in this notebook**, please [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7f63r3Yi5vE"
      },
      "source": [
        "## Objectives of this notebook üèÜ\n",
        "\n",
        "At the end of the notebook, you will:\n",
        "\n",
        "- Understand how works **ML-Agents**, the environment library.\n",
        "- Be able to **train agents in Unity Environments**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-mo_6rXIjRi"
      },
      "source": [
        "## Prerequisites üèóÔ∏è\n",
        "Before diving into the notebook, you need to:\n",
        "\n",
        "üî≤ üìö **Study [what is ML-Agents and how it works by reading Unit 5](https://huggingface.co/deep-rl-course/unit5/introduction)**  ü§ó  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYO1uD5Ujgdh"
      },
      "source": [
        "# Let's train our agents üöÄ\n",
        "\n",
        "**To validate this hands-on for the certification process, you just need to push your trained models to the Hub**. There‚Äôs no results to attain to validate this one. But if you want to get nice results you can try to attain:\n",
        "\n",
        "- For `Pyramids` : Mean Reward = 1.75\n",
        "- For `SnowballTarget` : Mean Reward = 15 or 30 targets hit in an episode.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DssdIjk_8vZE"
      },
      "source": [
        "## Set the GPU üí™\n",
        "- To **accelerate the agent's training, we'll use a GPU**. To do that, go to `Runtime > Change Runtime type`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" height='300' style=\"height:200px;\" alt=\"GPU Step 1\">\n",
        "<!-- height='300' has effect on colab-->\n",
        "<!-- style=\"height:200px\" has effect on github md rendering\"-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTfCXHy68xBv"
      },
      "source": [
        "- `Hardware Accelerator > GPU`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" style=\"height:200px;\" height=\"300\" alt=\"GPU Step 2\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV7qKOp1M3sK"
      },
      "source": [
        "## Clone the repository üîΩ\n",
        "\n",
        "- We need to clone the repository, that contains **ML-Agents.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9tlleT5M3sK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Clone the repository (can take 3min)\n",
        "!git clone --depth 1 https://github.com/Unity-Technologies/ml-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOb04ZY_M3sK"
      },
      "source": [
        "## Setup the Virtual Environment üîΩ\n",
        "- In order for the **ML-Agents** to run successfully in Colab,  Colab's Python version must meet the library's Python requirements.\n",
        "\n",
        "- We can check for the supported Python version under the `python_requires` parameter in the `setup.py` files. These files are required to set up the **ML-Agents** library for use and can be found in the following locations:\n",
        "  - `/content/ml-agents/ml-agents/setup.py`\n",
        "  - `/content/ml-agents/ml-agents-envs/setup.py`\n",
        "\n",
        "- Colab's Current Python version(can be checked using `!python --version`) doesn't match the library's `python_requires` parameter, as a result installation may silently fail and lead to errors like these, when executing the same commands later:\n",
        "  - `/bin/bash: line 1: mlagents-learn: command not found`\n",
        "  - `/bin/bash: line 1: mlagents-push-to-hf: command not found`\n",
        "\n",
        "- To resolve this, we'll create a virtual environment with a Python version compatible with the **ML-Agents** library.\n",
        "\n",
        "`Note:` *For future compatibility, always check the `python_requires` parameter in the installation files and set your virtual environment to the maximum supported Python version in the given below script if the Colab's Python version is not compatible*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0z7J3YpM3sK"
      },
      "outputs": [],
      "source": [
        "# Colab's Current Python Version (Incompatible with ML-Agents)\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Package 'mlagents' requires a different Python: 3.13.11 not in '<=3.10.12,>=3.10.1'"
      ],
      "metadata": {
        "id": "qOkXndFAUyfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install virtualenv and create a virtual environment\n",
        "!pip install virtualenv\n",
        "!virtualenv myenv\n",
        "\n",
        "# Download and install Miniconda\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local"
      ],
      "metadata": {
        "id": "vK_makyQVozP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accept terms of service\n",
        "!conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/main\n",
        "!conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/r"
      ],
      "metadata": {
        "id": "dkvr5OHISN0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zt0dXtpUM3sK"
      },
      "outputs": [],
      "source": [
        "# Activate Miniconda and install Python ver 3.10.12\n",
        "!source /usr/local/bin/activate\n",
        "!conda install -q -y --prefix /usr/local python=3.10.12 ujson  # Specify the version here\n",
        "\n",
        "# Set environment variables for Python and conda paths\n",
        "!export PYTHONPATH=/usr/local/lib/python3.10/site-packages/\n",
        "!export CONDA_PREFIX=/usr/local/envs/myenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkVnhjyaM3sK"
      },
      "outputs": [],
      "source": [
        "# Python Version in New Virtual Environment (Compatible with ML-Agents)\n",
        "!python --version\n",
        "#should be 3.10.xx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdpLHrnmM3sK"
      },
      "source": [
        "## Installing the dependencies üîΩ"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "# change notebooks directory (return to root)\n",
        "%cd /content/\n"
      ],
      "metadata": {
        "id": "BG9z20GFRjpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0plgC_FKM3sL"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Go inside the repository and install the package (can take 3min)\n",
        "%cd /content/ml-agents\n",
        "!pip3 install -e ./ml-agents-envs\n",
        "!pip3 install -e ./ml-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5_7Ptd_kEcG"
      },
      "source": [
        "## SnowballTarget ‚õÑ\n",
        "\n",
        "If you need a refresher on how this environments work check this section üëâ\n",
        "https://huggingface.co/deep-rl-course/unit5/snowball-target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRY5ufKUKfhI"
      },
      "source": [
        "### Download and move the environment zip file in `./training-envs-executables/linux/`\n",
        "- Our environment executable is in a zip file.\n",
        "- We need to download it and place it to `./training-envs-executables/linux/`\n",
        "- We use a linux executable because we use colab, and colab machines OS is Ubuntu (linux)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9Ls6_6eOKiA"
      },
      "outputs": [],
      "source": [
        "# Here, we create training-envs-executables and linux\n",
        "!mkdir -p ./training-envs-executables/linux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekSh8LWawkB5"
      },
      "source": [
        "Download the file SnowballTarget.zip from https://github.com/huggingface/Snowball-Target using `wget`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LosWO50wa77"
      },
      "outputs": [],
      "source": [
        "!wget \"https://github.com/huggingface/Snowball-Target/raw/main/SnowballTarget.zip\" -O ./training-envs-executables/linux/SnowballTarget.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LLVaEEK3ayi"
      },
      "source": [
        "Unzip the executable.zip file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FPx0an9IAwO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip -d ./training-envs-executables/linux/ ./training-envs-executables/linux/SnowballTarget.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyumV5XfPKzu"
      },
      "source": [
        "Make sure your file is accessible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdFsLJ11JvQf"
      },
      "outputs": [],
      "source": [
        "!chmod -R 755 ./training-envs-executables/linux/SnowballTarget"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -lah ./training-envs-executables/linux/\n"
      ],
      "metadata": {
        "id": "a8oOdeRcSfLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAuEq32Mwvtz"
      },
      "source": [
        "### Define the SnowballTarget config file\n",
        "- In ML-Agents, you define the **training hyperparameters into config.yaml files.**\n",
        "\n",
        "There are multiple hyperparameters. To know them better, you should check for each explanation with [the documentation](https://github.com/Unity-Technologies/ml-agents/blob/release_20_docs/docs/Training-Configuration-File.md)\n",
        "\n",
        "\n",
        "So you need to create a `SnowballTarget.yaml` config file in ./content/ml-agents/config/ppo/\n",
        "\n",
        "We'll give you here a first version of this config (to copy and paste into your `SnowballTarget.yaml file`), **but you should modify it**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "information about the config file might be found here:  \n",
        "https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md"
      ],
      "metadata": {
        "id": "atHUmbCYOVwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ml-agents/config/ppo/\n",
        "!pwd"
      ],
      "metadata": {
        "id": "vMeav62stdcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fname=\"SnowballTarget.yaml\"\n",
        "with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\"\"\n",
        "behaviors:\n",
        "  SnowballTarget:                    # behavior Name\n",
        "    trainer_type: ppo                # ppo: Proximal Policy Optimization\n",
        "    summary_freq: 10000\n",
        "    keep_checkpoints: 10             # number of checkpoints (.onnx)\n",
        "    checkpoint_interval: 50000       # checkpoint every n steps\n",
        "    max_steps: 200000                # total training steps\n",
        "    time_horizon: 64\n",
        "    threaded: false\n",
        "    hyperparameters:\n",
        "      learning_rate: 0.0003         # alpha: learning rate\n",
        "      learning_rate_schedule: linear\n",
        "      batch_size: 128\n",
        "      buffer_size: 2048\n",
        "      beta: 0.005                   # ppo specific param (entropy regularization strngth ML-agent: 0<x<0.01 )\n",
        "      epsilon: 0.2                  # Epsilon: exploration rate\n",
        "      lambd: 0.95\n",
        "      num_epoch: 3\n",
        "    network_settings:\n",
        "      normalize: false\n",
        "      hidden_units: 256\n",
        "      num_layers: 2\n",
        "      vis_encode_type: simple\n",
        "    reward_signals:                  # only use extrinsic reward (no curosity)\n",
        "      extrinsic:\n",
        "        gamma: 0.99                  # gamma: discount rate\n",
        "        strength: 1.0\n",
        "\"\"\")\n",
        "\n",
        "with open(fname, \"r\", encoding=\"utf-8\") as f:\n",
        "    print(f.read())\n"
      ],
      "metadata": {
        "id": "dAJ2K4djsZz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJJdo_5AyoGo"
      },
      "source": [
        "As an experimentation, you should also try to modify some other hyperparameters. Unity provides very [good documentation explaining each of them here](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md).\n",
        "\n",
        "Now that you've created the config file and understand what most hyperparameters do, we're ready to train our agent üî•."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "muF696nBZNKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9fI555bO12v"
      },
      "source": [
        "### Train the agent\n",
        "\n",
        "To train our agent, we just need to **launch mlagents-learn and select the executable containing the environment.**\n",
        "\n",
        "We define four parameters:\n",
        "\n",
        "1. `mlagents-learn <config>`: the path where the hyperparameter config file is.\n",
        "2. `--env`: where the environment executable is.\n",
        "3. `--run_id`: the name you want to give to your training run id.\n",
        "4. `--no-graphics`: to not launch the visualization during the training.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/mlagentslearn.png\" height='200' style=\"height:200px;\" alt=\"MlAgents learn\"/>\n",
        "\n",
        "Train the model and use the `--resume` flag to continue training in case of interruption.\n",
        "\n",
        "> It will fail first time if and when you use `--resume`, try running the block again to bypass the error.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN32oWF8zPjs"
      },
      "source": [
        "The training will take 10 to 35min depending on your config  \n",
        "<u>Note</u>: Mean rewards are written in console output as the training process goes on.\n",
        "\n",
        "https://www.immersivelimit.com/tutorials/reinforcement-learning-penguins-part-4-unity-ml-agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!git clone https://github.com/Unity-Technologies/ml-agents\n",
        "# Import binaries\n",
        "!mv ../evol.zip .\n",
        "!unzip evol.zip\n",
        "!pip install -e ./ml-agents/ml-agents\n"
      ],
      "metadata": {
        "id": "IubaRN7dPeDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the config file \"SnowballTarget.yaml\"\n",
        "#verify that the file is uploaded correctly\n",
        "!cat /content/ml-agents/config/ppo/SnowballTarget.yaml"
      ],
      "metadata": {
        "id": "Vf9nkG11YZ9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mlagents-learn /content/ml-agents/config/ppo/SnowballTarget.yaml \\\n",
        "--env=./training-envs-executables/linux/SnowballTarget/SnowballTarget \\\n",
        "--run-id=\"SnowballTarget1\" --no-graphics\n",
        "\n",
        "# pkg_resources is deprecated as an API."
      ],
      "metadata": {
        "id": "MjX_HnH9OS3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS-Yh1UdHfzy"
      },
      "outputs": [],
      "source": [
        "!mlagents-learn ./config/ppo/SnowballTarget.yaml --env=./training-envs-executables/linux/SnowballTarget/SnowballTarget --run-id=\"SnowballTarget1\" --no-graphics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vue94AzPy1t"
      },
      "source": [
        "### Push the agent to the ü§ó Hub\n",
        "\n",
        "- Now that we trained our agent, we‚Äôre **ready to push it to the Hub to be able to visualize it playing on your browserüî•.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izT6FpgNzZ6R"
      },
      "source": [
        "To be able to share your model with the community there are three more steps to follow:\n",
        "\n",
        "1Ô∏è‚É£ (If it's not already done) create an account to HF ‚û° https://huggingface.co/join\n",
        "\n",
        "2Ô∏è‚É£ Sign in and then, you need to store your authentication token from the Hugging Face website.\n",
        "- Create a new token (https://huggingface.co/settings/tokens) **with write role**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" height='150' style=\"height:150px;\" alt=\"Create HF Token\">\n",
        "\n",
        "- Copy the token\n",
        "- Run the cell below and paste the token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKt2vsYoK56o"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!huggingface-cli login\n",
        "#Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead."
      ],
      "metadata": {
        "id": "sfbKSB5CeuGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSU9qD9_6dem"
      },
      "source": [
        "If you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command instead: `huggingface-cli login`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK4fPfnczunT"
      },
      "source": [
        "Then, we simply need to run `mlagents-push-to-hf`.\n",
        "\n",
        "And we define 4 parameters:\n",
        "\n",
        "1. `--run-id`: the name of the training run id.\n",
        "2. `--local-dir`: where the agent was saved, it‚Äôs results/<run_id name>, so in my case results/First Training.\n",
        "3. `--repo-id`: the name of the Hugging Face repo you want to create or update. It‚Äôs always <your huggingface username>/<the repo name>\n",
        "If the repo does not exist **it will be created automatically**\n",
        "4. `--commit-message`: since HF repos are git repository you need to define a commit message.\n",
        "\n",
        "For instance:\n",
        "\n",
        "```sh\n",
        "!mlagents-push-to-hf  \\\n",
        "--run-id=\"SnowballTarget1\" \\\n",
        "--local-dir=\"./results/SnowballTarget1\" \\\n",
        "--repo-id=\"ThomasSimonini/ppo-SnowballTarget\"  \\\n",
        "--commit-message=\"First Push\"`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model should be located here...\n",
        "!ls -lah /content/ml-agents/results\n"
      ],
      "metadata": {
        "id": "muqXokn_eY9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#curl -LsSf https://hf.co/cli/install.sh | bash\n",
        "!hf auth login"
      ],
      "metadata": {
        "id": "0miyf4mrk_Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use %cd to move current notebook directory, and not just the subshell with !cd\n",
        "%cd /content/ml-agents/results/\n",
        "!pwd"
      ],
      "metadata": {
        "id": "2WvvWu4SlKSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example:\n",
        "\"\"\"\n",
        "mlagents-push-to-hf \\\n",
        "--run-id=\"SnowballTarget1\" \\\n",
        "--local-dir=\"./results/SnowballTarget1\" \\\n",
        "--repo-id=\"ThomasSimonini/ppo-SnowballTarget\" \\\n",
        "--commit-message=\"First Push\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RYsyQPt1hWg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAFzVB7OYj_H"
      },
      "outputs": [],
      "source": [
        "# if err:\n",
        "# File \"/usr/local/lib/python3.10/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n",
        "#    raise mapped_exc(message) from exc\n",
        "# httpx.ReadTimeout: The read operation timed out\n",
        "# be sure to locate the model directory properly...\n",
        "\n",
        "username=\"seb-835\"\n",
        "repo_name=\"ppo-SnowballTarget\"\n",
        "local_dir=\"SnowballTarget1\"\n",
        "\n",
        "!mlagents-push-to-hf \\\n",
        "--run-id=\"SnowballTarget1\" \\\n",
        "--local-dir=$local_dir \\\n",
        "--repo-id=$username/$repo_name \\\n",
        "--commit-message=\"first commit\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yborB0850FTM"
      },
      "source": [
        "Else, if everything worked you should have this at the end of the process(but with a different url üòÜ) :\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Your model is pushed to the hub. You can view your model here: https://huggingface.co/ThomasSimonini/ppo-SnowballTarget\n",
        "```\n",
        "\n",
        "It‚Äôs the link to your model, it contains a model card that explains how to use it, your Tensorboard and your config file. **What‚Äôs awesome is that it‚Äôs a git repository, that means you can have different commits, update your repository with a new push etc.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Uaon2cg0NrL"
      },
      "source": [
        "But now comes the best: **being able to visualize your agent online üëÄ.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMc4oOsE0QiZ"
      },
      "source": [
        "### Watch your agent playing üëÄ\n",
        "\n",
        "For this step it‚Äôs simple:\n",
        "\n",
        "1. Go here: https://huggingface.co/spaces/ThomasSimonini/ML-Agents-SnowballTarget\n",
        "\n",
        "2. Launch the game and put it in full screen by clicking on the bottom right button\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballtarget_load.png\" height='200' style=\"height:150px;\" alt=\"Snowballtarget load\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Djs8c5rR0Z8a"
      },
      "source": [
        "1. In step 1, type your username (your username is case sensitive: for instance, my username is ThomasSimonini not thomassimonini or ThOmasImoNInI) and click on the search button.\n",
        "\n",
        "2. In step 2, select your model repository.\n",
        "\n",
        "3. In step 3, **choose which model you want to replay**:\n",
        "  - I have multiple ones, since we saved a model every 500000 timesteps.\n",
        "  - But since I want the more recent, I choose `SnowballTarget.onnx`\n",
        "\n",
        "üëâ What‚Äôs nice **is to try with different models step to see the improvement of the agent.**\n",
        "\n",
        "And don't hesitate to share the best score your agent gets on discord in #rl-i-made-this channel üî•\n",
        "\n",
        "Let's now try a harder environment called Pyramids..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVMwRi4y_tmx"
      },
      "source": [
        "---\n",
        "\n",
        "# Pyramids üèÜ\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download, unzip and move the environment zip file in `./training-envs-executables/linux/`\n"
      ],
      "metadata": {
        "id": "S63-0tFsmuBK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2C48SGZjZYw"
      },
      "source": [
        "\n",
        "- Download the Pyramids.zip environment file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWh8Pl3sjZY2"
      },
      "outputs": [],
      "source": [
        "!wget \"https://huggingface.co/spaces/unity/ML-Agents-Pyramids/resolve/main/Pyramids.zip\" -O ./training-envs-executables/linux/Pyramids.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5LXPOPujZY3"
      },
      "source": [
        "- unzip the executable.zip file to the desired location\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmNgFdXhjZY3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip -d ./training-envs-executables/linux/ ./training-envs-executables/linux/Pyramids.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1jxwhrJjZY3"
      },
      "source": [
        "- Make sure the directory has read permissions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fDd03btjZY3"
      },
      "outputs": [],
      "source": [
        "!chmod -R 755 ./training-envs-executables/linux/Pyramids/Pyramids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqceIATXAgih"
      },
      "source": [
        "###  Modify the PyramidsRND config file\n",
        "- Contrary to the first environment which was a custom one, **Pyramids was made by the Unity team**.\n",
        "- Therefore, PyramidsRND config file exists and is in ./content/ml-agents/config/ppo/PyramidsRND.yaml\n",
        "- What does \"RND\" in PyramidsRND Means ?  \n",
        "RND stands for <b><font color='crimson'>Random Network Distillation</font></b>.  \n",
        "It's a way to generate curiosity rewards.  \n",
        "For more information on this technique, please read: https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938\n",
        "\n",
        "For the training, one thing has to be modified:\n",
        "- The total training steps hyperparameter is too high, since we can hit the benchmark (mean reward = 1.75) in only 1M training steps.  \n",
        "-> In the following file: <font color=cyan>config/ppo/PyramidsRND.yaml</font>. modify <font color='magenta'>max_steps</font>: <font color=darkviolet>1000000</font>.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids-config.png\" height='200' style=\"height:150px;\" alt=\"Pyramids config\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI-5aPL7BWVk"
      },
      "source": [
        "As an experiment,  \n",
        "try to modify some other hyperparameters.  \n",
        "Unity provides very good [documentation](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md) on this topic.\n",
        "\n",
        "... now we're ready to train our agent üî•."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5hr1rvIBdZH"
      },
      "source": [
        "### Train the agent\n",
        "<i>The training will take 30 to 45min depending on your machine</i>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXi4-IaHBhqD"
      },
      "outputs": [],
      "source": [
        "!mlagents-learn ./config/ppo/PyramidsRND.yaml --env=./training-envs-executables/linux/Pyramids/Pyramids --run-id=\"Pyramids Training\" --no-graphics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txonKxuSByut"
      },
      "source": [
        "### Push the agent to the ü§ó Hub\n",
        "\n",
        "- Now that we trained our agent, we‚Äôre **ready to push it to the Hub to be able to visualize it playing on your browserüî•.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#curl -LsSf https://hf.co/cli/install.sh | bash\n",
        "!hf auth login"
      ],
      "metadata": {
        "id": "cLAT0YD9sFy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use %cd to move current notebook directory, and not just the subshell with !cd\n",
        "%cd /content/ml-agents/results/\n",
        "!pwd"
      ],
      "metadata": {
        "id": "dmPIrRF_sFy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiEQbv7rB4mU"
      },
      "outputs": [],
      "source": [
        "!mlagents-push-to-hf  --run-id= # Add your run id  --local-dir= # Your local dir  --repo-id= # Your repo id  --commit-message= # Your commit message"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#example:\n",
        "\"\"\"\n",
        "mlagents-push-to-hf \\\n",
        "--run-id=\"SnowballTarget1\" \\\n",
        "--local-dir=\"./results/SnowballTarget1\" \\\n",
        "--repo-id=\"ThomasSimonini/ppo-SnowballTarget\" \\\n",
        "--commit-message=\"First Push\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "XaZIrD7jsFy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aZfgxo-CDeQ"
      },
      "source": [
        "### Watch your agent playing üëÄ\n",
        "\n",
        "üëâ https://huggingface.co/spaces/unity/ML-Agents-Pyramids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGG_oq2n0wjB"
      },
      "source": [
        "# üéÅ Bonus: Why not train on another environment?\n",
        "Now that you know how to train an agent using MLAgents, **why not try another environment?**\n",
        "\n",
        "MLAgents provides 17 different and we‚Äôre building some custom ones. The best way to learn is to try things of your own, have fun.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSAkJxSr0z6-"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/1400/0*xERdThTRRM2k_U9f.png\" height='350' style=\"height:200px;\" alt=\"Pyramids config\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiyF4FX-04JB"
      },
      "source": [
        "You have the full list of the Unity official environments here üëâ https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Examples.md\n",
        "\n",
        "For the demos to visualize your agent üëâ https://huggingface.co/unity\n",
        "\n",
        "For now we have integrated:\n",
        "- [Worm](https://huggingface.co/spaces/unity/ML-Agents-Worm) demo where you teach a **worm to crawl**.\n",
        "- [Walker](https://huggingface.co/spaces/unity/ML-Agents-Walker) demo where you teach an agent **to walk towards a goal**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI6dPWmh064H"
      },
      "source": [
        "That‚Äôs all for today. Congrats on finishing this tutorial!\n",
        "\n",
        "The best way to learn is to practice and try stuff. Why not try another environment? ML-Agents has 17 different environments, but you can also create your own? Check the documentation and have fun!\n",
        "\n",
        "See you on Unit 6 üî•,\n",
        "\n",
        "## Keep Learning, Stay  awesome ü§ó"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}